{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a195dd47",
   "metadata": {},
   "source": [
    "# Deploy model and run inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03b56d7",
   "metadata": {},
   "source": [
    "Now that you have trained successfully your model, you want to look at the predictions on some sample images. To do so, you will need to find the s3 path of the exported model. You can navigate to the Training jobs section of the AWS web UI and click on the training job of interest. Scroll down and you should see something like this:\n",
    "\n",
    "![Example Artefact](../data/example_artefact.png)\n",
    "\n",
    "The model artefact path should look something like \n",
    "```s3://sagemaker-us-east-1-073338978050/tf2-object-detection-2022-10-22-21-26-37-033/output/model.tar.gz```. Use this value to update the `model_artefact` variable below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b3785b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow import TensorFlowModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6ae306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Update the model artefact here. \n",
    "model_artefact = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c71be7",
   "metadata": {},
   "source": [
    "Now we can deploy the model. Run the following cell and check that the model was correctly deployed by navigating to Inference endpoints in the web UI.\n",
    "\n",
    "![Example endpoint](../data/example_endpoints.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c3496d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TensorFlowModel(\n",
    "    name=name_from_base('tf2-object-detection'),\n",
    "    model_data=model_artefact,\n",
    "    role=role,\n",
    "    framework_version='2.8'\n",
    ")\n",
    "\n",
    "predictor = model.deploy(initial_instance_count=1, instance_type='ml.g4dn.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f3d9cf",
   "metadata": {},
   "source": [
    "## Run inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d2b7b4",
   "metadata": {},
   "source": [
    "Our model is now deployed and we can query it. We are going to use the images available in `data/test_video` to run inference and generate a video. To do so, we are going to need a few tools:\n",
    "* we need to sort all the frames by index order (which corresponds to chronological order)\n",
    "* we need a function to load images into numpy array\n",
    "* we need a loop to run inference and display the results on the input image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470edb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "import matplotlib\n",
    "from matplotlib import animation\n",
    "\n",
    "import visualization_utils as viz_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e2d725",
   "metadata": {},
   "source": [
    "We list the frame paths and sort them by index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2656b900",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames_path = sorted(glob.glob('../data/test_video/*.png'), \n",
    "                     key = lambda k: int(os.path.basename(k).split('.')[0].split('_')[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4dd116",
   "metadata": {},
   "source": [
    "We create a small function to load images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c4dc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(path: str) -> np.ndarray:\n",
    "    \"\"\" This function reads an image from the path and returns a numpy array\"\"\"\n",
    "    cv_img = cv2.imread(path,1).astype('uint8')\n",
    "    cv_img = cv2.cvtColor(cv_img, cv2.COLOR_BGR2RGB)\n",
    "    return cv_img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca4a04f",
   "metadata": {},
   "source": [
    "We create a mapping from id to name for visualization purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1535d202",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_index = {1:{'id': 1, 'name': 'vehicle'}, \n",
    "                  2: {'id': 2, 'name': 'pedestrian'},\n",
    "                  4: {'id': 4, 'name': 'cyclist'}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d323e30",
   "metadata": {},
   "source": [
    "This is the main loop:\n",
    "* we load images to numpy\n",
    "* we query the deployed model\n",
    "* we display the inference results on the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a491b0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "for idx, path in enumerate(frames_path):\n",
    "    if idx % 10 == 0:\n",
    "        print(f'Processed {idx}/{len(frames_path)} images.')\n",
    "        \n",
    "    # load image\n",
    "    img = image_file_to_tensor(path)\n",
    "    inputs = {'instances': [img.tolist()]}\n",
    "    \n",
    "    # run inference and extract results\n",
    "    detections = predictor.predict(inputs)['predictions'][0]\n",
    "    detection_boxes = np.array(detections['detection_boxes'])\n",
    "    detection_classes = [int(x) for x in detections['detection_classes']]\n",
    "    detection_scores = detections['detection_scores']\n",
    "    \n",
    "    # display results on image\n",
    "    image_np_with_detections = \\\n",
    "        viz_utils.visualize_boxes_and_labels_on_image_array(\n",
    "            img,\n",
    "            detection_boxes,\n",
    "            detection_classes,\n",
    "            detection_scores,\n",
    "            category_index,\n",
    "            use_normalized_coordinates=True,\n",
    "            max_boxes_to_draw=100,\n",
    "            min_score_thresh=0.6,\n",
    "            agnostic_mode=False)\n",
    "    images.append(image_np_with_detections)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ccfab78",
   "metadata": {},
   "source": [
    "We can verify that the model worked correctly by displaying elements of the `images` list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47b0a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(images[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260a1ea0",
   "metadata": {},
   "source": [
    "Finally, we can create a gif with our detections by running the following function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba9c141",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure()\n",
    "f.subplots_adjust(left=0, bottom=0, right=1, top=1, wspace=None, hspace=None)\n",
    "ax = plt.subplot(111)\n",
    "ax.axis('off')\n",
    "im_obj = ax.imshow(images[0])\n",
    "\n",
    "def animate(idx):\n",
    "    image = images[idx]\n",
    "    im_obj.set_data(image)\n",
    "\n",
    "anim = animation.FuncAnimation(f, animate, frames=len(images))\n",
    "anim.save('animation.gif', fps=5, dpi=300, writer='PillowWriter')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b2effe",
   "metadata": {},
   "source": [
    "You should see an `animation.gif` file created in the same folder as this notebook. This is the kind of results you should see by using the model provided in `pipeline.config` file.\n",
    "\n",
    "![Result Example](../data/animation.gif)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
